{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5133f9-0d25-41cc-a3f2-881282adbe56",
   "metadata": {},
   "source": [
    "<center><h4>EXPERIMENT 3</h4></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16710783-a1d7-472e-a0b6-d7ea3e70ea7c",
   "metadata": {},
   "source": [
    "<center><h4>IMAGE CAPTIONING USING FLICKR8K DATASET</h4></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e23fdd5-e3ba-4669-ab7f-08be4203a399",
   "metadata": {},
   "source": [
    "#### AIM:\n",
    "To develop and implement a deep learning based image captioning model using the Flickr8k dataset, where a Convolutional Neural Network (CNN) extracts image features and a Recurrent Neural Network (RNN) with LSTM units generates natural language descriptions for the given images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a248d052-0b8c-47c2-b663-85cdd47f303f",
   "metadata": {},
   "source": [
    "#### PRE-REQUISITES:\n",
    "1. Basics of Machine Learning\n",
    "2. Python Programming\n",
    "3. Knowledge on Numpy, Pandas, Matplotlib, TensorFlow/ Keras\n",
    "4. Jupyter Notebook\n",
    "5. Data Pre-Processing Techniques\n",
    "6. Knowledge on Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14529670-38ac-4524-ac28-737be1332115",
   "metadata": {},
   "source": [
    "#### FLICKR8K Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c065bff-14f3-4da0-89b8-84d25c0bed88",
   "metadata": {},
   "source": [
    "- Flikr8k Dataset contains 8,000 images, each showing people or animals doing various activities.\n",
    "- Each image has five different captions written in plain English to describe it.\n",
    "- The dataset is mainly used for training and testing image captioning models.\n",
    "- Download the Dataset - https://www.kaggle.com/datasets/adityajn105/flickr8k/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02602361-8fc2-4159-9967-74042b59bfde",
   "metadata": {},
   "source": [
    "### 1. Importing the Basic Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a80513c1-9461-454b-8ac3-c44eaae751b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data handling and analysis\n",
    "import numpy as np                 \n",
    "import pandas as pd                \n",
    "import matplotlib.pyplot as plt    \n",
    "import os                          \n",
    "import string \n",
    "import re\n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de315551-6cb8-41ae-9dad-115e6facfb07",
   "metadata": {},
   "source": [
    "### 2. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c384740-183b-42dd-b3ca-862e97c66ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = \"/Users/srinutupakula/Desktop/Deep Learning Lab/Experiment 3/archive/Images\"\n",
    "captions_path = '/Users/srinutupakula/Desktop/Deep Learning Lab/Experiment 3/archive/captions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea5f0411-e492-401b-b66f-7eeb98a8b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Funtion to load the Captions File (File Handling)\n",
    "\n",
    "def load_doc(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Load captions text\n",
    "captions_text = load_doc(captions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "891f390f-5fb8-4020-9b63-a15e3f541a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\\n1000268201_693b08cb0e.jpg,A girl going into a wooden building .\\n1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .\\n1000268201_693b08cb0e.jpg,A little girl climbing the stairs to her playhouse .\\n1000268201_693b08cb0e.jpg,A little girl in a pink dress going into a wooden cabin .\\n1001773457_577c3a7d70.jpg,A black dog and a spotted dog are fighting\\n1001773457_577c3a7d70.jpg,A bl'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the captions text\n",
    "captions_text[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e71a99-19d9-428f-917b-efcc5d99508d",
   "metadata": {},
   "source": [
    "### 3. Pre-Process the Captions Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4f1fa-684c-4e46-b60b-a1cb68c6fa83",
   "metadata": {},
   "source": [
    "#### a) Load captions file and organize in a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634bc43-121a-4f2a-91b6-051261800b0f",
   "metadata": {},
   "source": [
    "- It reads the captions file, then groups captions by their image name in a dictionary. Each image name becomes a key, and its value is a list of all captions for that image. It splits each line at the first comma, trims spaces, and stores the captions clearly for quick access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99196693-a969-4f3e-a670-c848ed1d5edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_captions(captions_path):\n",
    "    captions_dict = {}\n",
    "    with open(captions_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Split only on the first comma --> filename, caption\n",
    "            img_id, caption = line.split(',', 1)\n",
    "\n",
    "            img_id = img_id.strip()\n",
    "            caption = caption.strip()\n",
    "\n",
    "            captions_dict.setdefault(img_id, []).append(caption)\n",
    "    return captions_dict\n",
    "\n",
    "captions = load_captions(captions_path)\n",
    "len(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a3da874-557b-492f-9cd1-64997db6c095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       " 'A girl going into a wooden building .',\n",
       " 'A little girl climbing into a wooden playhouse .',\n",
       " 'A little girl climbing the stairs to her playhouse .',\n",
       " 'A little girl in a pink dress going into a wooden cabin .']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions['1000268201_693b08cb0e.jpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70338571-bec4-4367-ad86-9f3affd9b100",
   "metadata": {},
   "source": [
    "#### b) Clean captions (lowercase, remove punctuation/numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776241ae-a244-46d1-811f-0b889ff02a4e",
   "metadata": {},
   "source": [
    "- Cleans the captions for each image by making all letters lowercase, removing punctuation and numbers, and taking care of extra spaces. It processes every caption in the dictionary and returns a new dictionary with the cleaned captions, making the text neat and consistent for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20abe1e5-ed06-4838-96d3-2644735246b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_captions(captions_dict):\n",
    "    cleaned_dict = {}\n",
    "    for img_id, captions in captions_dict.items():\n",
    "        cleaned_captions = []\n",
    "        for cap in captions:\n",
    "            # Lowercase\n",
    "            cap = cap.lower()\n",
    "            # Remove punctuation and numbers\n",
    "            cap = re.sub(r'[^a-z\\s]', '', cap)\n",
    "            # Remove extra spaces\n",
    "            cap = re.sub(r'\\s+', ' ', cap).strip()\n",
    "            cleaned_captions.append(cap)\n",
    "        cleaned_dict[img_id] = cleaned_captions\n",
    "    return cleaned_dict\n",
    "\n",
    "captions = clean_captions(captions)\n",
    "len(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6dc7c9d5-9f30-41ab-838d-55f15b2183a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a child in a pink dress is climbing up a set of stairs in an entry way',\n",
       " 'a girl going into a wooden building',\n",
       " 'a little girl climbing into a wooden playhouse',\n",
       " 'a little girl climbing the stairs to her playhouse',\n",
       " 'a little girl in a pink dress going into a wooden cabin']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions['1000268201_693b08cb0e.jpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb0275-c54f-407e-84cb-6d5e2d72e7bd",
   "metadata": {},
   "source": [
    "#### c) Add \\<start> and \\<end> tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7da2230-dc13-472a-aab0-a467bf693ec9",
   "metadata": {},
   "source": [
    "- Adds special tokens *start* and *end* to every caption in the dictionary. For each image, it takes its captions and wraps them with these tokens, then returns a new dictionary. These tokens help mark the beginning and end of a caption, which is useful when training models for image captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2e3bd15-e524-44c7-906c-a8ad5195cca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_tokens(captions_dict):\n",
    "    tokenized_dict = {}\n",
    "    for img_id, captions in captions_dict.items():\n",
    "        tokenized_dict[img_id] = [f\"<start> {cap} <end>\" for cap in captions]\n",
    "    return tokenized_dict\n",
    "\n",
    "captions = add_tokens(captions)\n",
    "len(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5ee7545-3c43-4936-b7c7-526376f7f8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> a child in a pink dress is climbing up a set of stairs in an entry way <end>',\n",
       " '<start> a girl going into a wooden building <end>',\n",
       " '<start> a little girl climbing into a wooden playhouse <end>',\n",
       " '<start> a little girl climbing the stairs to her playhouse <end>',\n",
       " '<start> a little girl in a pink dress going into a wooden cabin <end>']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions['1000268201_693b08cb0e.jpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa8554c-bc4f-41bf-b512-883b17f5b77b",
   "metadata": {},
   "source": [
    "#### d) Build vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b5e713-21ae-4d67-b6f3-e84c2875e221",
   "metadata": {},
   "source": [
    "- It reates a vocabulary set from all the captions in the dictionary. It goes through each caption, splits it into individual words, and adds them to a set (which automatically removes duplicates). The final result is a collection of all unique words used across the captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06611f11-d628-4310-87ad-99140c11aa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8780"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocabulary(captions_dict):\n",
    "    vocab = set()\n",
    "    for captions in captions_dict.values():\n",
    "        for cap in captions:\n",
    "            vocab.update(cap.split())\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocabulary(captions)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "11a4d597-8792-4a20-85dd-1805e02c7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9742d1-7d2f-4621-87f5-5778244d35db",
   "metadata": {},
   "source": [
    "#### e) Create (image_id, caption) pairs for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2819c28e-c531-4397-ba1e-e102234ca83c",
   "metadata": {},
   "source": [
    "- It takes the captions dictionary and creates a list of (image_id, caption) pairs. It loops through each image ID and its associated captions, then for every caption, it makes a tuple with the image ID and that caption. The result is a flat list where each entry links a single image to one of its captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aef5abd5-4332-4a15-b72e-e50f09423a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40455"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_image_caption_pairs(captions_dict):\n",
    "    pairs = []\n",
    "    for img_id, captions in captions_dict.items():\n",
    "        for cap in captions:\n",
    "            pairs.append((img_id, cap))\n",
    "    return pairs\n",
    "\n",
    "pairs = create_image_caption_pairs(captions)\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "425c8d69-d8e9-4344-9ad5-76eab90672f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1000268201_693b08cb0e.jpg',\n",
       "  '<start> a child in a pink dress is climbing up a set of stairs in an entry way <end>'),\n",
       " ('1000268201_693b08cb0e.jpg',\n",
       "  '<start> a girl going into a wooden building <end>'),\n",
       " ('1000268201_693b08cb0e.jpg',\n",
       "  '<start> a little girl climbing into a wooden playhouse <end>'),\n",
       " ('1000268201_693b08cb0e.jpg',\n",
       "  '<start> a little girl climbing the stairs to her playhouse <end>'),\n",
       " ('1000268201_693b08cb0e.jpg',\n",
       "  '<start> a little girl in a pink dress going into a wooden cabin <end>')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4d84b-d661-4f64-a78e-e8f2d4f4d956",
   "metadata": {},
   "source": [
    "### 4. Extract Image Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d37b7-1d2f-4f71-9573-93cecfff56af",
   "metadata": {},
   "source": [
    "#### a) Choose a pre-trained CNN and create an encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad1855d-db90-47b9-a056-fe58e86b3637",
   "metadata": {},
   "source": [
    "- It Imports InceptionV3 model pre-trained on ImageNet and builds an image encoder by removing the top classification layers (include_top=False). It uses global average pooling (pooling='avg') to produce a fixed-size feature vector for each input image. This encoder will be used to extract meaningful features from images for the captioning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4856818d-315c-4920-bfdb-a54aa1420120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_image_encoder():\n",
    "    base = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "    return base\n",
    "\n",
    "encoder = build_image_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e92c8c7d-93ec-48be-b69f-37c7dfd615be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07009088-7c2d-4649-8d60-12811fed7943",
   "metadata": {},
   "source": [
    "#### b) Image Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0709ca-0826-4604-bef0-13ef84673dcf",
   "metadata": {},
   "source": [
    "- It loads an image from the given images_path and resizes it to the target size (default 299×299 pixels, matching InceptionV3’s input size). It converts the image to a NumPy array and removes the alpha channel if present (some PNGs have 4 channels). Then it adds a batch dimension (required for model input) and applies model-specific preprocessing (scaling and normalization) before returning the processed image array ready for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e48c39d1-ae67-481d-95e4-34da1d4eff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(images_path, target_size=(299, 299)):\n",
    "    img = load_img(images_path, target_size=target_size)         \n",
    "    arr = img_to_array(img)                               \n",
    "    if arr.shape[-1] == 4:                                \n",
    "        arr = arr[..., :3]\n",
    "    arr = np.expand_dims(arr, axis=0)                     \n",
    "    arr = preprocess_input(arr)                          \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fe6852-bdfb-4feb-8e51-b9f77644bfca",
   "metadata": {},
   "source": [
    "#### c) Extract features for all images and save them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb34dd7-5046-4288-81e8-a129ba179b0c",
   "metadata": {},
   "source": [
    "- It extracts feature vectors from all images in a given folder using a pre-trained CNN encoder (like InceptionV3). It processes only common image formats (jpg, jpeg, png), loads and preprocesses each image, obtains its feature vector by running it through the encoder, and stores these features in a dictionary keyed by filename. Optionally, it saves the extracted features as a pickle file for later use. Finally, it runs the function on your images folder and saves the features to \"features.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ab9def54-d157-48c6-a8b8-69de5913e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_features_from_folder(images_path, encoder, save_path=None):\n",
    "    features = {}\n",
    "    \n",
    "    # Only process common image types\n",
    "    img_files = [f for f in os.listdir(images_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    for fname in tqdm(img_files, desc='Extracting features'):\n",
    "        full = os.path.join(images_path, fname)\n",
    "        try:\n",
    "            # Load and preprocess\n",
    "            img_arr = load_and_preprocess_image(full)\n",
    "            # Extract features\n",
    "            feat = encoder.predict(img_arr, verbose=0)\n",
    "            features[fname] = feat.flatten()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {fname}: {e}\")\n",
    "    \n",
    "    # Save features if path provided\n",
    "    if save_path:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(features, f)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b60db0e8-1362-4928-a547-7227394615a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████████████| 8091/8091 [10:59<00:00, 12.27it/s]\n"
     ]
    }
   ],
   "source": [
    "#features = extract_features_from_folder(images_path, encoder, \"features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1703827f-afee-4896-8668-2abea017fdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images processed: 8091\n",
      "Sample image: 2387197355_237f6f41ee.jpg\n",
      "Feature vector shape: (2048,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total images processed: {len(features)}\")\n",
    "first_key = list(features.keys())[0]\n",
    "print(f\"Sample image: {first_key}\")\n",
    "print(f\"Feature vector shape: {features[first_key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a76c46-455a-436b-bead-f280da704832",
   "metadata": {},
   "source": [
    "#### d) Create Image Feature and Caption Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403f916-cf2c-4f6d-92fa-2ac8cba48e99",
   "metadata": {},
   "source": [
    "- This code creates a list of training pairs, where each pair consists of an image feature vector and one of its corresponding captions. It iterates over each image ID in the captions dictionary, checks if the image’s features exist, and if so, pairs each caption with the image features. If features for an image are missing, it prints a warning. Finally, it returns the list of all such (feature, caption) pairs and prints the total count of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7157a29b-507d-4ebd-824b-94a5b0b60b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40455"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_training_pairs(features, captions_dict):\n",
    "    pairs = []\n",
    "    for img_id, caps in captions_dict.items():\n",
    "        if img_id in features:\n",
    "            for cap in caps:\n",
    "                pairs.append((features[img_id], cap))\n",
    "        else:\n",
    "            print(f\"Warning: No features found for image {img_id}\")\n",
    "    return pairs\n",
    "\n",
    "training_pairs = create_training_pairs(features, captions)\n",
    "len(training_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96adf88f-0dd7-44b9-a203-4f9cb764a190",
   "metadata": {},
   "source": [
    "#### e) Tokenize Captions and Prepare Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b49b18d-2b63-4854-a2d7-453a6ce82e7b",
   "metadata": {},
   "source": [
    "- It processes all the captions by first gathering them into one list and then using a tokenizer to convert each word into a unique integer. The tokenizer is fitted on all captions to build a vocabulary, including a special token for unknown words. It calculates the total vocabulary size and converts every caption into a sequence of integer tokens. To ensure uniform input size for the model, it finds the longest caption length and pads all shorter sequences with zeros at the end, so every caption sequence has the same length. This prepares the text data in a numerical form suitable for training the deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a5a3295b-66c7-424d-8bb8-f5ffbc8edb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 8780\n",
      "Maximum caption length: 37\n"
     ]
    }
   ],
   "source": [
    "# Convert captions (strings) into sequences of integers that the model can process\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Gather all captions in a list\n",
    "all_captions = []\n",
    "for caps in captions.values():\n",
    "    all_captions.extend(caps)\n",
    "\n",
    "# Initialize and fit tokenizer on all captions\n",
    "tokenizer = Tokenizer(oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "\n",
    "# Vocabulary size (add 1 for padding token)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "# Convert captions to sequences\n",
    "sequences = tokenizer.texts_to_sequences(all_captions)\n",
    "\n",
    "# Find max caption length for padding\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "print(f\"Maximum caption length: {max_length}\")\n",
    "\n",
    "# Pad sequences to max_length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dca9bd-94dd-44b1-9c1c-627df4e93acb",
   "metadata": {},
   "source": [
    "#### f) Prepare Final Dataset for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d279d-0a64-4a8d-95cf-22d35495800a",
   "metadata": {},
   "source": [
    "- It prepares the training data by turning each caption into sequences of words where the model learns to predict the next word. For every caption, it creates multiple input-output pairs: the input is the image features plus a partial caption, and the output is the next word in the caption. The input sequences are padded to the same length, and the output words are one-hot encoded to indicate the correct prediction. All these inputs and outputs are collected into arrays that can be used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1cf18f5e-e0b8-4fb2-a9eb-0123981e626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate image features and captions into arrays\n",
    "\n",
    "# image features\n",
    "X1 = [] \n",
    "# input sequences (captions shifted)\n",
    "X2 = [] \n",
    "# output word (next word to predict)\n",
    "y = []   \n",
    "\n",
    "for img_feat, cap in training_pairs:\n",
    "    seq = tokenizer.texts_to_sequences([cap])[0]\n",
    "    for i in range(1, len(seq)):\n",
    "        in_seq, out_seq = seq[:i], seq[i]\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n",
    "\n",
    "        out_seq_onehot = np.zeros(vocab_size)\n",
    "        out_seq_onehot[out_seq] = 1 \n",
    "\n",
    "        X1.append(img_feat)\n",
    "        X2.append(in_seq)\n",
    "        y.append(out_seq_onehot)\n",
    "\n",
    "\n",
    "X1 = np.array(X1)\n",
    "X2 = np.array(X2)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2edd4509-4333-4fd3-aee3-657bea9fc0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((476793, 2048), (476793, 37), (476793, 8780))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape, X2.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53708c8b-eab3-41e1-ac3b-a71210c5773c",
   "metadata": {},
   "source": [
    "### 5. Build the Decoder Model (Caption Generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acd4d8a-4cd9-4211-8198-a0ffc12e3fb9",
   "metadata": {},
   "source": [
    "- It builds an image captioning model that takes both image features and caption sequences as input. The image features are processed through dropout and a dense layer, while the captions pass through an embedding layer, dropout, and an LSTM to understand the sequence. These two outputs are combined and fed through additional dense layers to predict the next word in the caption. The model uses categorical cross-entropy loss and the Adam optimizer, and includes an early stopping mechanism to stop training if the loss stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6b7e0166-67ab-4311-aadb-e0938f17da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Image feature input\n",
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# Sequence input\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# Decoder (combine)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "# Define the Call back\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "80755af6-e634-4ce4-8672-e3f250e327eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,247,680</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8780</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,256,460</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m2,247,680\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m524,544\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m525,312\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8780\u001b[0m)      │  \u001b[38;5;34m2,256,460\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,619,788</span> (21.44 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,619,788\u001b[0m (21.44 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,619,788</span> (21.44 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,619,788\u001b[0m (21.44 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04265a37-3c01-437e-bf40-e3f802ccaefc",
   "metadata": {},
   "source": [
    "### 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "584f8e32-e534-48de-a23d-0f0380047b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m6705/6705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m605s\u001b[0m 90ms/step - loss: 4.2663 - val_loss: 3.3908\n",
      "Epoch 2/15\n",
      "\u001b[1m6705/6705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 74ms/step - loss: 3.1816 - val_loss: 3.2879\n",
      "Epoch 3/15\n",
      "\u001b[1m6705/6705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1660s\u001b[0m 248ms/step - loss: 2.9390 - val_loss: 3.2546\n",
      "Epoch 4/15\n",
      "\u001b[1m6705/6705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m731s\u001b[0m 109ms/step - loss: 2.8113 - val_loss: 3.2650\n",
      "Epoch 5/15\n",
      "\u001b[1m6705/6705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m576s\u001b[0m 86ms/step - loss: 2.7291 - val_loss: 3.3035\n",
      "Epoch 6/15\n",
      "\u001b[1m6705/6705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 73ms/step - loss: 2.6744 - val_loss: 3.3484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1697588e0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10% of the training data will be set aside for validation\n",
    "# The model doesn’t train on this data but uses it to check how well it’s generalizing after each epoch\n",
    "\n",
    "model.fit([X1, X2], y, epochs=15, batch_size=64, verbose=1, validation_split=0.1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef6fac3-ade2-470c-b4f9-4ba8decdb238",
   "metadata": {},
   "source": [
    "### 7. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "959dc843-4180-427d-a5bb-b3942ddad2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to reuse it later without retraining\n",
    "model.save('image_captioning_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a7754-9f3f-408c-be59-768cd53cab92",
   "metadata": {},
   "source": [
    "### 8. Define a function to generate captions for new Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf5c43-8b53-44d4-846b-8eaeadf08dfe",
   "metadata": {},
   "source": [
    "- This function generates a caption for a given image feature vector using the trained model and tokenizer. It starts with the special <start> token and iteratively predicts the next word by converting the current text sequence into integers, padding it, and feeding it along with the image feature to the model. At each step, it selects the word with the highest predicted probability and adds it to the caption. The process continues until the model predicts the <end> token or reaches the maximum caption length. Finally, it returns the complete generated caption as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0598a7dd-725c-4650-a2ec-0c4322e63869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, tokenizer, photo_feature, max_length):\n",
    "    in_text = '<start>'\n",
    "    for _ in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n",
    "        yhat = model.predict([photo_feature.reshape(1, -1), sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = tokenizer.index_word.get(yhat, None)\n",
    "        if word is None or word == 'end':\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "    return in_text.replace('<start> ', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dd83d3-0e7f-4980-9fda-fdc80e5dc33d",
   "metadata": {},
   "source": [
    "### 9. Generate Captions for New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e7af99f5-1af1-4ca4-a7e4-e07f892a910c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|████████████████████████| 2/2 [00:00<00:00, 11.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract the Features for new images\n",
    "features = extract_features_from_folder(images_path, encoder, \"features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a6c15cd1-44d1-4cf7-bcb8-2653c92a4c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image feature for a new image (extract features as before)\n",
    "new_image_feature = features['kids.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d8314e94-2fc6-4393-8074-d2846d9bee99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a man in a red shirt is running on a field'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption = generate_caption(model, tokenizer, new_image_feature, max_length)\n",
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "07d163cd-e00e-4dcf-9580-ee0de0390948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image feature for a new image (extract features as before)\n",
    "new_image_feature = features['dog.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a1924344-3a6b-482a-8970-9c355df99686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a dog is running through a field'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption = generate_caption(model, tokenizer, new_image_feature, max_length)\n",
    "caption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f538c1-f946-457e-93fc-445487700c26",
   "metadata": {},
   "source": [
    "#### RESULT:\n",
    "The image captioning model was developed to generate captions for input images by combining visual features extracted from a pre-trained CNN with sequence modeling of captions using an LSTM network. Using the Flickr8k dataset, the model was able to produce descriptive sentences that capture the main elements and actions in images, demonstrating the effectiveness of deep learning and transfer learning techniques for this task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
